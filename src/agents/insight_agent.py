import json
from datetime import datetime
from pathlib import Path
from src.utils.llm import call_llm_model
from src.utils.logger import log_step


class InsightAgent:
    """
    Insight Agent
    --------------
    Uses the data summary and an LLM model to generate structured hypotheses
    explaining potential factors behind ROAS (Return on Ad Spend) changes.
    """

    def __init__(self, config):
        self.config = config
        self.summary_path = Path("reports/data_summary.json")
        self.output_path = Path("reports/insights.json")
        self.prompt_path = Path("prompts/insight_prompt.md")
        self.model = config.get("model", "gemini-2.0-flash")

    def load_data_summary(self):
        """Load the JSON summary generated by the DataAgent."""
        log_step("InsightAgent", "Data Loading", "Loading data summary file.")
        try:
            with open(self.summary_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            log_step(
                "InsightAgent",
                "Data Loading",
                f"Loaded summary successfully. Keys: {list(data.keys())}"
            )
            return data
        except FileNotFoundError:
            log_step("InsightAgent", "Data Loading Error", f"File not found: {self.summary_path}")
        except json.JSONDecodeError:
            log_step("InsightAgent", "Data Loading Error", "Invalid JSON format in summary file.")
        except Exception as e:
            log_step("InsightAgent", "Data Loading Error", f"Unexpected error: {e}")
        return {}

    def _load_prompt(self):
        """Read the LLM reasoning prompt."""
        try:
            return self.prompt_path.read_text(encoding="utf-8")
        except FileNotFoundError:
            log_step("InsightAgent", "Prompt Error", "insight_prompt.md not found.")
        except Exception as e:
            log_step("InsightAgent", "Prompt Error", f"Error reading prompt file: {e}")
        return None

    def _run_llm(self, prompt, summary):
        """Call the configured LLM with structured input."""
        try:
            combined_prompt = f"{prompt}\n\nData Summary:\n{json.dumps(summary, indent=2)}"
            log_step("InsightAgent", "LLM Execution", f"Calling model: {self.model}")
            response = call_llm_model(
                model=self.model,
                prompt=combined_prompt,
                temperature=0.7,
            )
            return json.loads(response)
        except json.JSONDecodeError:
            log_step("InsightAgent", "LLM Error", "LLM returned invalid JSON. Using fallback logic.")
        except Exception as e:
            log_step("InsightAgent", "LLM Error", f"Error calling LLM: {e}")
        return None

    def _rule_based(self, summary):
        """Generate fallback hypotheses if LLM or prompt fails."""
        hypotheses = []
        roas_trend = summary.get("roas_trend", {}).get("trend_direction")

        if roas_trend == "decline":
            hypotheses = [
                {
                    "id": "H1",
                    "title": "Ad fatigue reducing CTR and conversions",
                    "evidence": (
                        "ROAS declining while spend and impressions remain stable; "
                        "CTR is low across multiple campaigns."
                    ),
                    "confidence": 0.8,
                },
                {
                    "id": "H2",
                    "title": "Audience targeting misalignment",
                    "evidence": (
                        "Low CTR observed across demographics, suggesting a mismatch "
                        "between creative messaging and audience interests."
                    ),
                    "confidence": 0.65,
                },
            ]
        return {"hypotheses": hypotheses}

    def run(self):
        """Execute the insight generation pipeline."""
        summary = self.load_data_summary()
        if not summary:
            print("InsightAgent terminated: No valid data summary available.")
            return {}

        prompt = self._load_prompt()
        if not prompt:
            log_step("InsightAgent", "Fallback", "Using rule-based insight generation.")
            result_json = self._rule_based(summary)
        else:
            result_json = self._run_llm(prompt, summary)
            if not result_json:
                log_step("InsightAgent", "Fallback", "Falling back to rule-based insights.")
                result_json = self._rule_based(summary)

        insights_output = {
            "timestamp": datetime.now().isoformat(),
            "hypotheses": result_json.get("hypotheses", []),
            "summary_reference": str(self.summary_path),
        }

        # Ensure output directory exists
        try:
            self.output_path.parent.mkdir(parents=True, exist_ok=True)
            with open(self.output_path, "w", encoding="utf-8") as f:
                json.dump(insights_output, f, indent=2)
            print(f"Insights successfully saved to {self.output_path}")
        except Exception as e:
            log_step("InsightAgent", "File Save Error", f"Failed to save insights: {e}")

        return insights_output