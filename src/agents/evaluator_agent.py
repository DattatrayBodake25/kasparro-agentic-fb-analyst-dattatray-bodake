import json
import random
from datetime import datetime
from pathlib import Path


class EvaluatorAgent:
    """
    EvaluatorAgent
    ---------------
    Evaluates hypotheses generated by the Insight Agent using 
    basic quantitative logic to validate and prioritize causes.
    """

    def __init__(self, config):
        self.config = config
        self.insights_path = Path("reports/insights.json")
        self.summary_path = Path("reports/data_summary.json")

    def load_inputs(self):
        """Load insights and data summary from the reports directory."""
        try:
            with open(self.insights_path, "r", encoding="utf-8") as f:
                insights = json.load(f)
            with open(self.summary_path, "r", encoding="utf-8") as f:
                summary = json.load(f)

            # Handle edge case if JSON is accidentally stringified
            if isinstance(insights, str):
                insights = json.loads(insights)
            if isinstance(summary, str):
                summary = json.loads(summary)

            print("[EvaluatorAgent] Inputs loaded successfully.")
            return insights, summary

        except FileNotFoundError as e:
            print(f"[EvaluatorAgent] Required input file missing: {e}")
            return {}, {}
        except json.JSONDecodeError as e:
            print(f"[EvaluatorAgent] Invalid JSON format in input: {e}")
            return {}, {}
        except Exception as e:
            print(f"[EvaluatorAgent] Error loading inputs: {e}")
            return {}, {}

    def validate_hypotheses(self, insights, summary):
        """Validate hypotheses using simple rules and trends."""
        results = []

        for hyp in insights.get("hypotheses", []):
            base_conf = hyp.get("confidence", 0.5)
            roas_trend = summary.get("roas_trend", {}).get("trend_direction", "neutral")
            low_ctr_count = summary.get("low_ctr_summary", {}).get("count", 0)

            # Apply heuristic validation
            if "fatigue" in hyp.get("title", "").lower() and low_ctr_count > 100:
                validation_conf = min(base_conf + 0.1, 1.0)
                reason = "Strong evidence: many low CTR campaigns detected."
            elif "competition" in hyp.get("title", "").lower():
                validation_conf = round(base_conf + random.uniform(-0.05, 0.05), 2)
                reason = "Moderate evidence: indirect signs via declining ROAS but stable CTR."
            else:
                validation_conf = round(base_conf + random.uniform(-0.1, 0.1), 2)
                reason = "Limited supporting evidence; requires further validation."

            results.append({
                "id": hyp.get("id"),
                "title": hyp.get("title", "Unnamed Hypothesis"),
                "original_confidence": base_conf,
                "validated_confidence": validation_conf,
                "reasoning": reason,
                "validated": validation_conf > 0.6
            })

        return results

    def run(self):
        """Main entry point for hypothesis evaluation."""
        insights, summary = self.load_inputs()
        if not insights:
            print("[EvaluatorAgent] No insights found. Evaluation skipped.")
            return {}

        validated = self.validate_hypotheses(insights, summary)

        output = {
            "timestamp": datetime.now().isoformat(),
            "validated_hypotheses": validated
        }

        # Ensure reports directory exists
        Path("reports").mkdir(parents=True, exist_ok=True)

        try:
            with open("reports/evaluation_results.json", "w", encoding="utf-8") as f:
                json.dump(output, f, indent=2, ensure_ascii=False)
            print(f"[EvaluatorAgent] {len(validated)} hypotheses validated successfully.")
            print("[EvaluatorAgent] Results saved to reports/evaluation_results.json")
        except Exception as e:
            print(f"[EvaluatorAgent] Error saving evaluation results: {e}")

        return output